#-----------------------------------------------
#   HCM benchmark (2022)       
#   Author: Pedro Barbosa
#
#   Script to compare performance of tools 
#   across different datasets and rank the
#   top tools according to the metric desired
#-----------------------------------------------
import argparse
import pandas as pd

def average_by_datasets(df):
    df = df.groupby('tool').agg({'rank': 'mean'}).sort_values('rank')
    print("Top 10 tools:")
    print('\n'.join([x for x in df.head(10).index]))
    
def average_by_metrics(df: pd.DataFrame, metrics: list):
    metrics = ['weighted_{}'.format(x) if x in ['precision', 'auROC', 'pr_auROC'] else x for x in metrics]
    df = df[['tool', 'dataset'] + metrics].copy()
    
    # Average metric values
    df = df.set_index(['tool', 'dataset']).mean(axis=1).reset_index()
    df = df.rename(columns={0: 'value'})

    # Generate ranks per dataset
    df = df.groupby('dataset').apply(lambda x: x.sort_values('value', ascending=False)).reset_index(drop=True)
    df['rank'] = df.groupby('dataset').cumcount() + 1

    #_df = df.pivot(index='tool',columns='dataset')[['weighted_auROC','weighted_accuracy']]
    #df.to_csv('stats.tsv', sep="\t")
    return df 
    
def get_data(files: list, dataset_names: list = None):
    dfs = []
    for i, f in enumerate(files):
        _df = pd.read_csv(f, sep="\t")
        
        if dataset_names is not None:
            _df['dataset'] = dataset_names[i]
        else:
            _df['dataset'] = i
        
        _df['weighted_auROC'] = _df.auROC * _df.coverage
        _df['weighted_pr_auROC'] = _df.pr_auROC * _df.coverage
        _df['weighted_precision'] = _df.precision * _df.coverage
        dfs.append(_df)
    
    return pd.concat(dfs)
  
def main():
    parser = argparse.ArgumentParser(add_help=True)
    parser.add_argument('--metrics_files', metavar="", required=True, nargs='+', help='Statistics file generated by a VETA run. Mininum of two files is required.')
    parser.add_argument('--ranking_metrics', metavar="", nargs='+', choices=['weighted_F1', 'weighted_accuracy', 'weighted_norm_mcc', 'norm_mcc', 'precision', 'auROC', 'pr_auROC'])
    parser.add_argument('--dataset_names', metavar="", nargs='+', help="Name of each dataset. Must correspond to the order of 'metrics' files")
    args = parser.parse_args()
    
    if len(args.metrics_files) < 2:
        raise ValueError('At least two metric files are required.')

    if args.dataset_names:
        if len(args.metrics_files) != len(args.dataset_names):
            raise ValueError("--dataset_names must have the same number of names as of the --metrics_files.")

    df = get_data(args.metrics_files, args.dataset_names)

    df = average_by_metrics(df, args.ranking_metrics)
    average_by_datasets(df)
    
if __name__ == "__main__":
    main()
